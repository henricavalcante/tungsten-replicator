#################################
# REPLICATOR.PROPERTIES.ARCHIVE #
#################################
#
# This file contains properties for MySQL replication.
#
# NOTE TO ALL USERS:  Blank property values are assigned as empty strings. 
# To assign the default value, comment out the key=value assignment.
#
# NOTE TO WINDOWS USERS:  Single backslash characters are treated as escape 
# characters.  You must use forward slash (/) or double backslashes in file 
# names. 

#################################
# GLOBAL REPLICATOR PARAMETERS  #
#################################

# Replicator role.  You must specify a value that corresponds to a 
# pipeline name like master, slave, direct, etc.  There is no default 
# for this value--it must be set or the replicator will not go online.  
replicator.role=@{REPL_ROLE}

# URI to which we connect when this replicator is a slave.   
replicator.master.connect.uri=@{REPL_MASTER_URI}

# URI for our listener when we are acting as a master.  Slaves 
# use this as their connect URI.
replicator.master.listen.uri=@{REPL_THL_PROTOCOL}://@{HOST.HOST}:@{REPL_SVC_THL_PORT}/

# Replicator auto-enable.  If true, replicator automatically goes online 
# at start-up time. 
replicator.auto_enable=@{REPL_AUTOENABLE}

# Replicator auto-recovery.  If greater than 0, the replicator service
# attempts to go back online up to this number of times before failing.
replicator.autoRecoveryMaxAttempts=@{REPL_AUTO_RECOVERY_MAX_ATTEMPTS}

# Replicator auto-recovery delay.  How long to wait before going online.
# Settings are milliseconds by default.  You can also apply settings in
# seconds, minutes, hours, and days, as in 10s, 30m, 2h, etc. This
# setting allows time for resources to clean up or recover after a failure.
replicator.autoRecoveryDelayInterval=@{REPL_AUTO_RECOVERY_DELAY_INTERVAL}

# Replicator auto-recover reset interval.  If the replicator is online for
# this amount of time, reset the count of auto-recovery attempts.  This value
# may need to be large for services that commit large blocks as apply errors
# may take a while to show up after going online again.
replicator.autoRecoveryResetInterval=@{REPL_AUTO_RECOVERY_RESET_INTERVAL}

# Source ID.  Identifies the replication event source.  It must be 
# unique for each replicator node and normally is the host name. 
# Do not use values like localhost or 127.0.0.1.
replicator.source_id=@{EXTRACTOR.REPL_DBHOST}

# Site to which the replicator belongs.
site.name=default

# Cluster name to which the replicator belongs.
cluster.name=@{CLUSTERNAME}

# Replication service type.  Values are 'remote' or 'local'.  Remote 
# remote services are bringing in transactions from another master. 
replicator.service.type=@{REPL_SVC_SERVICE_TYPE}

# Name of this replication service.  
service.name=@{SERVICE.DEPLOYMENT_SERVICE}

# Whether to allow comments in SQL statements to show the service name.
# This must be set to avoid multi-master replication loops if updates
# from one master go into the binlog of another master.
replicator.service.comments=@{SERVICE.REPL_SVC_ENABLE_MASTER_SERVICE_COMMENTS}

# Name of the local replication service.  This parameter must be set when 
# performing bi-directional replication using a remote slave.  Events 
# generated by this service are dropped, thereby preventing replication 
# loops.  It may not be the same as the value of service.name if the service
# is remote. 
local.service.name=@{SERVICE.DSNAME}

# Global queue size for pipelines.  This defines the number of events
# buffered between stages.  Values greater than 1 improve performance
# dramatically but mean that you need to have enough heap memory to
# handle blobs and large transaction fragments.
replicator.global.buffer.size=@{REPL_BUFFER_SIZE}

##########################
# OPEN REPLICATOR PLUGIN #
##########################

# Available OpenReplicator providers
replicator.plugin.tungsten=com.continuent.tungsten.replicator.management.tungsten.TungstenPlugin
replicator.plugin.script=com.continuent.tungsten.replicator.management.script.ScriptPlugin

# Chosen OpenReplicator provider
replicator.plugin=tungsten

#################################
# REPLICATOR PIPELINES          #
#################################

# Pipeline stages use block commit to commit multiple source transactions
# simultaneously.  Block commit is controlled by the following settings on 
# stages:
#   blockCommitRowCount -- Commit when this number of transactions is 
#   reached or stage runs out of in-coming transactions
#   blockCommitInterval -- Commit when time interval elapses *or* number of
#   transactions for blockCommitRowCount is reached.
#   blockCommitPolicy -- Values are strict and lax. 
#
# If neither block commit setting is in effect, we commit transactions
# individually.  Interval settings are milliseconds by default.  You can also
# apply settings in seconds, minutes, hours, and days, as in 10s, 30m, 2h, 
# or 1d respectively. 
#
# Strict block commit forces a commit on fragmented events, service changes
# and events marked as unsafe for block commit.  It is default and required
# for correct operation on RDBMS.  Lax block commit ignores these hence allows
# consistently large blocks.  It is the best setting for data warehouse 
# loading where updates are idempotent (e.g., Hadoop). 

# Generic pipelines.
replicator.pipelines=archive

# ARCHIVE PIPELINE:  three stages:  extract from remote THL to local THL;
# extract from local THL to queue; apply from queue to DBMS.
replicator.pipeline.archive=remote-to-thl
replicator.pipeline.archive.stores=thl
replicator.pipeline.archive.services=
replicator.pipeline.archive.syncTHLWithExtractor=false

replicator.stage.remote-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.remote-to-thl.extractor=thl-remote
replicator.stage.remote-to-thl.applier=thl-applier
replicator.stage.remote-to-thl.filters=

#################################
# TRANSACTION HISTORY LOG (THL) #
#################################

# NOTE:  If you run multiple replication services, beware of collisions 
# on log file names and THL lister ports. 

# Set the THL storage implementation class.  All replicators now use
# a single implementation. 
replicator.store.thl=com.continuent.tungsten.replicator.thl.THL

# THL implementations require database access to store metadata.
replicator.store.thl.dataSource=

# Uncomment the following properties to control disk log storage location
# and size of files.  These are default values.  NOTE:  naming the logs by 
# service name is critical to prevent replication services from overwriting
# other service logs. 
replicator.store.thl.log_dir=@{SERVICE.REPL_LOG_DIR}
replicator.store.thl.log_file_size=@{REPL_THL_LOG_FILE_SIZE}

# Force replicator to check if THL is in sync with database.
# This check is enabled by default. Set the following property
# to false in order to disable the check.
replicator.store.thl.logConsistencyCheck=true

# The following property tells the THL server how long, as a maximum,
# in milliseconds, to block during an accept() call.  This interval, in turn
# will be directly factored into the amount of time a replicator will
# take to transition to the offline state on systems whose
# kernels or Java runtime won't allow for the interruption of a blocking
# accept() call.  If this interval is too short, it may result in problems 
# with new client connections, and if too long, will result in excessively
# slow transitions to offline etc.
replicator.thl.server.accept.timeout=5000

# Change the following property to control the buffer size used for THL
# I/O operations.  The default of 128k seems to work well. 
replicator.store.thl.bufferSize=131072

# The flush interval is the number of milliseconds that log writes may delay
# before being forced to storage.  0 means that every flush call flushes 
# immediately.  Larger values can help buffer writes but add latency to 
# pipelines.  This value only takes effect when fsyncOnFlush=true. 
replicator.store.thl.flushIntervalMillis=500

# Flush operations normally release writes to the OS but do
# not force them to storage.  You may request a full fsync on each flush by
# setting the following property to true.  This makes log updates more
# durable.  If you do this the flush interval should be relatively large to
# avoid impacting overall throughput or you should put the log on  
# high-performance storage to reduce fsync overhead.
replicator.store.thl.fsyncOnFlush=@{REPL_THL_LOG_FSYNC}

# To drop log files after a certain period, set the retention to an interval
# which is <number>{d|h|m|s}, where the letters stand for days, hours, minutes,
# or seconds respectively.  If unset logs are retained indefinitely.
replicator.store.thl.log_file_retention=@{REPL_THL_LOG_RETENTION}

# The THL serialization for events is pluggable.  The default is Protobuf
# serialization which is relatively fast and compact.  Java serialization
# is also provided but is experimental.
replicator.store.thl.event_serializer=com.continuent.tungsten.replicator.thl.serializer.ProtobufSerializer

# The disk log can compute checksums automatically on log records.  This
# is enabled by default but can cut performance by 50% or more.  Set false
# for faster log performance or to suppress reading *and* writing of checksums. 
replicator.store.thl.doChecksum=@{REPL_THL_DO_CHECKSUM}

# Maximum number of events to transfer at once.  Higher values are better
# but as with queue store sizes require more memory.
replicator.thl.protocol.buffer_size=10

# THL listener address for remote access.  To listen on all interfaces,
# use a host value of 0.0.0.0, as in thl://0.0.0.0:2112/.  The port
# defaults to 2112 if you do not specify a value.  To enable SSL use
# 'thls' instead of 'thl'.  Note that clients must exactly match the
# scheme (thl vs. thls), host, and port to connect. 
replicator.store.thl.storageListenerUri=@{REPL_THL_PROTOCOL}://0.0.0.0:@{REPL_SVC_THL_PORT}

# The transfer rate between the master and the slave can be improved
# by setting the following setting to a value greater than 1, which
# is the default.  In doing so, this will also require more memory
# on the master side.
replicator.store.thl.resetPeriod=1

# Do not allow this replication service to make any changes to the THL files
replicator.store.thl.readOnly=@{REPL_SVC_THL_READ_ONLY}

##############
# EXTRACTORS #
##############

# Remote THL extractor.  
replicator.extractor.thl-remote=com.continuent.tungsten.replicator.thl.RemoteTHLExtractor
replicator.extractor.thl-remote.connectUri=${replicator.master.connect.uri}

# If true, check to ensure logs are consistent on connection to master by 
# comparing epoch numbers of last committed transaction.  This can be 
# temporarily overridden when going online using 'trepctl online -force'.
replicator.extractor.thl-remote.checkSerialization=true

# Set requested interval in milliseconds for heartbeat events from remote THL.
# This may need to be adjusted higher if masters are very slow to respond or
# to scan the THL for slave start positions. 
replicator.extractor.thl-remote.heartbeatInterval=3000

# Set the number of seconds between retrying to master(s) after a lost 
# connection. 
replicator.extractor.thl-remote.retryInterval=1

# Set preferred THL server role.  Common values are master, slave, or empty 
# (i.e., no value).  When replicating from a Tungsten cluster 'slave' avoids
# problems with failover when transactions are trapped on an old master. 
replicator.extractor.thl-remote.preferredRole=@{REPL_MASTER_PREFERRED_ROLE}

# Set the number of seconds to find the preferred role when reconnecting.  
# After this expires the replicator will choose any available master. 
replicator.extractor.thl-remote.preferredRoleTimeout=30

############
# APPLIERS #
############

# Local THL applier. 
replicator.applier.thl-applier=com.continuent.tungsten.replicator.thl.THLStoreApplier
replicator.applier.thl-applier.storeName=thl

###########
# FILTERS # 
###########

@{includeAll(REPL_SVC_FILTER_CONFIG)}

#####################
# PIPELINE SERVICES #
#####################

########
# NONE #
########

################################
# BACKUP/RESTORE CONFIGURATION #
################################

########
# NONE #
########

####################################################
# ERROR-HANDLING AND CONSISTENCY-CHECKING POLICIES #
####################################################

# How to react on extractor failure. Possible values are 'stop' or 'warn'. 
replicator.extractor.failure_policy=stop

# How to react on applier failure. Possible values are 'stop' or 'warn'. 
replicator.applier.failure_policy=stop

# How applier should react when detecting a row update that did not change anything.
# Possible values are 'stop', 'warn' or 'ignore'. 
replicator.applier.failOnZeroRowUpdate=@{REPL_SVC_FAIL_ON_ZERO_ROW_UPDATE}

# How to react on consistency check failure.  Possible values are 'stop' or 
# 'warn'. 
replicator.applier.consistency_policy=@{REPL_CONSISTENCY_POLICY}

# Should consistency check be sensitive to column names and/or types? Settings
# on a slave must be identical to master's. Values are 'true' or 'false'.
replicator.applier.consistency_column_names=true
replicator.applier.consistency_column_types=true
